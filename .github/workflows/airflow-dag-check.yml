# GitHub Actions Airflow DAG Validation Pipeline for games
# Comprehensive Apache Airflow workflow testing and validation
# Generated from template - ensures DAG integrity and reliability
#
# Validation stages:
# 1. DAG Syntax and Import Validation
# 2. DAG Structure and Dependency Analysis
# 3. Task Configuration Validation
# 4. Integration Testing with Airflow Components
# 5. Performance and Resource Analysis
# 6. Documentation and Metadata Validation

name: Airflow DAG Validation

# ============================================================================
# TRIGGER CONFIGURATION
# ============================================================================

on:
  # Trigger on changes to Airflow-related files
  push:
    branches:
      - main
      - develop
      - feature/airflow-*
    paths:
      - 'airflow/**'
      - 'dags/**'
      - 'plugins/**'
      - 'requirements*airflow*.txt'
      - 'pyproject.toml'
      - '.github/workflows/airflow-*.yml'

  # Trigger on pull requests affecting Airflow
  pull_request:
    branches:
      - main
      - develop
    paths:
      - 'airflow/**'
      - 'dags/**'
      - 'plugins/**'
      - 'requirements*airflow*.txt'
      - 'pyproject.toml'

  # Manual workflow dispatch for testing
  workflow_dispatch:
    inputs:
      airflow_version:
        description: 'Airflow version to test against'
        required: false
        default: '3.0.1'
        type: choice
        options:
          - '3.0.1'
          - '3.0.0'
          - '2.10.3'
          - '2.10.2'
          - '2.9.3'
      test_mode:
        description: 'Test execution mode'
        required: false
        default: 'full'
        type: choice
        options:
          - 'full'
          - 'syntax-only'
          - 'integration-only'
      dag_filter:
        description: 'DAG filter pattern (regex)'
        required: false
        default: '.*'
        type: string

  # Scheduled validation (daily at 3 AM UTC)
  schedule:
    - cron: '0 3 * * *'

# ============================================================================
# GLOBAL CONFIGURATION
# ============================================================================

# Concurrency control for Airflow validation
concurrency:
  group: airflow-validation-${{ github.ref }}
  cancel-in-progress: true

# Global environment variables
env:
  # Airflow configuration (optimized for Airflow 3.x)
  AIRFLOW_HOME: ${{ github.workspace }}/airflow
  AIRFLOW__CORE__LOAD_EXAMPLES: 'False'
  AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS: 'False'
  AIRFLOW__CORE__UNIT_TEST_MODE: 'True'
  AIRFLOW__CORE__DAGS_FOLDER: ${{ github.workspace }}/airflow/dags
  AIRFLOW__CORE__PLUGINS_FOLDER: ${{ github.workspace }}/airflow/plugins
  AIRFLOW__WEBSERVER__EXPOSE_CONFIG: 'False'
  AIRFLOW__WEBSERVER__RBAC: 'True'
  AIRFLOW__CORE__ENABLE_XCOM_PICKLING: 'False'
  AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG: '16'
  AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG: '16'
  
  # Database configuration (PostgreSQL 16)
  AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: 'postgresql+psycopg2://airflow:airflow@localhost:5432/airflow'
  AIRFLOW__CORE__EXECUTOR: 'SequentialExecutor'
  
  # Security configuration (enhanced for 3.x)
  AIRFLOW__WEBSERVER__SECRET_KEY: 'test-secret-key-for-ci'
  AIRFLOW__CORE__FERNET_KEY: 'test-fernet-key-for-ci-validation'
  AIRFLOW__WEBSERVER__EXPOSE_HOSTNAME: 'False'
  AIRFLOW__WEBSERVER__EXPOSE_STACKTRACE: 'False'
  
  # Logging configuration
  AIRFLOW__LOGGING__LOGGING_LEVEL: 'WARNING'
  AIRFLOW__CORE__COLORED_CONSOLE_LOG: 'False'
  AIRFLOW__LOGGING__COLORED_LOG: 'False'
  
  # Performance optimizations for Airflow 3.x
  AIRFLOW__SCHEDULER__PARSING_PROCESSES: '2'
  AIRFLOW__CORE__PARALLELISM: '32'
  AIRFLOW__CORE__DAG_CONCURRENCY: '16'
  
  # UV and Python configuration
  UV_CACHE_DIR: /tmp/.uv-cache
  PYTHONPATH: ${{ github.workspace }}/airflow/dags:${{ github.workspace }}/airflow/plugins:$PYTHONPATH
  PYTHONDONTWRITEBYTECODE: 1
  PYTHONUNBUFFERED: 1

# ============================================================================
# JOB DEFINITIONS
# ============================================================================

jobs:
  # ==========================================================================
  # DAG SYNTAX VALIDATION JOB
  # ==========================================================================
  
  dag-syntax-validation:
    name: DAG Syntax Validation
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    strategy:
      matrix:
        python-version: ['3.10', '3.11']
        airflow-version: 
          - ${{ inputs.airflow_version || '3.0.1' }}
    
    outputs:
      dag-count: ${{ steps.dag-discovery.outputs.dag-count }}
      dag-list: ${{ steps.dag-discovery.outputs.dag-list }}
      
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 1
          
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
          
      - name: Install UV
        uses: astral-sh/setup-uv@v3
        with:
          enable-cache: true
          
      - name: Install Airflow and dependencies
        run: |
          # Install core dependencies
          uv sync --dev
          
          # Install Airflow with specific version
          uv add "apache-airflow==${{ matrix.airflow-version }}"
          
          # Install modern Airflow providers (compatible with 3.x)
          uv add apache-airflow-providers-postgres
          uv add apache-airflow-providers-http
          uv add apache-airflow-providers-ftp
          uv add apache-airflow-providers-email
          uv add apache-airflow-providers-slack
          uv add apache-airflow-providers-common-sql
          uv add apache-airflow-providers-fab
          
          # Install testing dependencies
          uv add pytest-airflow
          
      - name: Initialize Airflow environment
        run: |
          # Create Airflow directories
          mkdir -p $AIRFLOW_HOME/dags $AIRFLOW_HOME/logs $AIRFLOW_HOME/plugins
          
          # Copy project DAGs to Airflow directory
          if [ -d "airflow/dags" ]; then
            cp -r airflow/dags/* $AIRFLOW_HOME/dags/ 2>/dev/null || true
          fi
          if [ -d "dags" ]; then
            cp -r dags/* $AIRFLOW_HOME/dags/ 2>/dev/null || true
          fi
          
          # Copy plugins if they exist
          if [ -d "airflow/plugins" ]; then
            cp -r airflow/plugins/* $AIRFLOW_HOME/plugins/ 2>/dev/null || true
          fi
          if [ -d "plugins" ]; then
            cp -r plugins/* $AIRFLOW_HOME/plugins/ 2>/dev/null || true
          fi
          
          # Initialize Airflow database
          uv run airflow db init
          
      - name: Discover DAGs
        id: dag-discovery
        run: |
          echo "::group::DAG Discovery"
          
          # List all Python files in DAGs directory
          dag_files=$(find $AIRFLOW_HOME/dags -name "*.py" -type f | wc -l)
          echo "Found $dag_files Python files in DAGs directory"
          
          # Get DAG list from Airflow
          dag_list=$(uv run airflow dags list --output json 2>/dev/null | jq -r '.[].dag_id' | tr '\n' ',' | sed 's/,$//')
          dag_count=$(echo "$dag_list" | tr ',' '\n' | wc -l)
          
          if [ -z "$dag_list" ]; then
            dag_count=0
            dag_list=""
          fi
          
          echo "dag-count=$dag_count" >> $GITHUB_OUTPUT
          echo "dag-list=$dag_list" >> $GITHUB_OUTPUT
          
          echo "Discovered $dag_count DAGs: $dag_list"
          echo "::endgroup::"
          
      - name: Python syntax validation
        run: |
          echo "::group::Python Syntax Validation"
          
          # Check Python syntax for all DAG files
          find $AIRFLOW_HOME/dags -name "*.py" -type f -exec python -m py_compile {} \;
          
          # Check plugin files if they exist
          if [ -d "$AIRFLOW_HOME/plugins" ]; then
            find $AIRFLOW_HOME/plugins -name "*.py" -type f -exec python -m py_compile {} \;
          fi
          
          echo "âœ… All Python files have valid syntax"
          echo "::endgroup::"
          
      - name: DAG import validation
        run: |
          echo "::group::DAG Import Validation"
          
          # Test DAG imports using Airflow's DagBag
          uv run python -c "
          import sys
          from airflow.models import DagBag
          
          print('Loading DAGs from: $AIRFLOW_HOME/dags')
          dag_bag = DagBag(dag_folder='$AIRFLOW_HOME/dags', include_examples=False)
          
          if dag_bag.import_errors:
              print('âŒ DAG import errors found:')
              for filename, stacktrace in dag_bag.import_errors.items():
                  print(f'Error in {filename}:')
                  print(stacktrace)
                  print('-' * 50)
              sys.exit(1)
          else:
              print(f'âœ… Successfully loaded {len(dag_bag.dags)} DAGs without import errors')
              for dag_id in dag_bag.dags.keys():
                  print(f'  - {dag_id}')
          "
          
          echo "::endgroup::"
          
      - name: DAG structure validation
        run: |
          echo "::group::DAG Structure Validation"
          
          # Validate DAG structure and configuration
          uv run python -c "
          from airflow.models import DagBag
          import sys
          
          dag_bag = DagBag(dag_folder='$AIRFLOW_HOME/dags', include_examples=False)
          errors = []
          warnings = []
          
          for dag_id, dag in dag_bag.dags.items():
              print(f'Validating DAG: {dag_id}')
              
              # Check required attributes
              if not dag.description:
                  warnings.append(f'{dag_id}: Missing description')
              
              if not dag.owner:
                  warnings.append(f'{dag_id}: Missing owner')
              
              if not dag.tags:
                  warnings.append(f'{dag_id}: No tags specified')
              
              # Check for start_date
              if not dag.start_date:
                  errors.append(f'{dag_id}: Missing start_date')
              
              # Check for schedule
              if dag.schedule_interval is None and dag.timetable is None:
                  warnings.append(f'{dag_id}: No schedule specified')
              
              # Check task count
              task_count = len(dag.task_ids)
              if task_count == 0:
                  errors.append(f'{dag_id}: No tasks defined')
              elif task_count > 50:
                  warnings.append(f'{dag_id}: High task count ({task_count}), consider optimization')
              
              # Check for circular dependencies
              try:
                  dag.partial_subset(task_ids_or_regex=dag.task_ids, include_downstream=False, include_upstream=False)
              except Exception as e:
                  errors.append(f'{dag_id}: Circular dependency detected: {e}')
          
          # Report results
          if warnings:
              print('âš ï¸  Warnings found:')
              for warning in warnings:
                  print(f'  {warning}')
          
          if errors:
              print('âŒ Errors found:')
              for error in errors:
                  print(f'  {error}')
              sys.exit(1)
          else:
              print('âœ… All DAGs passed structure validation')
          "
          
          echo "::endgroup::"

  # ==========================================================================
  # DAG TESTING JOB
  # ==========================================================================
  
  dag-testing:
    name: DAG Testing
    runs-on: ubuntu-latest
    timeout-minutes: 25
    needs: dag-syntax-validation
    if: needs.dag-syntax-validation.outputs.dag-count > 0
    
    strategy:
      matrix:
        airflow-version: 
          - ${{ inputs.airflow_version || '3.0.1' }}
    
    services:
      postgres:
        image: postgres:16
        env:
          POSTGRES_USER: airflow
          POSTGRES_PASSWORD: airflow
          POSTGRES_DB: airflow
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          
      - name: Install UV
        uses: astral-sh/setup-uv@v3
        with:
          enable-cache: true
          
      - name: Install Airflow with PostgreSQL support
        run: |
          uv sync --dev
          uv add "apache-airflow==${{ matrix.airflow-version }}"
          uv add "apache-airflow[postgres]"
          uv add apache-airflow-providers-postgres
          uv add apache-airflow-providers-common-sql
          uv add pytest-airflow
          
      - name: Configure Airflow with PostgreSQL
        run: |
          # Override database configuration for PostgreSQL 16
          export AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="postgresql+psycopg2://airflow:airflow@localhost:5432/airflow"
          export AIRFLOW__CORE__EXECUTOR="LocalExecutor"
          
          # Create Airflow directories
          mkdir -p $AIRFLOW_HOME/dags $AIRFLOW_HOME/logs $AIRFLOW_HOME/plugins
          
          # Copy DAGs and plugins
          if [ -d "airflow/dags" ]; then
            cp -r airflow/dags/* $AIRFLOW_HOME/dags/
          fi
          if [ -d "airflow/plugins" ]; then
            cp -r airflow/plugins/* $AIRFLOW_HOME/plugins/
          fi
          
          # Initialize Airflow with PostgreSQL 16
          uv run airflow db init
          
          # Create admin user for Airflow 3.x
          uv run airflow users create \
            --username admin \
            --firstname Admin \
            --lastname User \
            --role Admin \
            --email admin@games.com \
            --password admin
          
        env:
          AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@localhost:5432/airflow
          AIRFLOW__CORE__EXECUTOR: LocalExecutor
          
      - name: Run DAG tests
        run: |
          echo "::group::DAG Unit Tests"
          
          # Run any existing DAG tests
          if [ -d "airflow/tests" ] || [ -d "tests/dags" ]; then
            echo "Running DAG unit tests..."
            if [ -d "airflow/tests" ]; then
              uv run pytest airflow/tests/ -v --tb=short
            fi
            if [ -d "tests/dags" ]; then
              uv run pytest tests/dags/ -v --tb=short
            fi
          else
            echo "No DAG tests found, running basic validation tests"
          fi
          
          echo "::endgroup::"
          
        env:
          AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@localhost:5432/airflow
          
      - name: Test DAG execution
        run: |
          echo "::group::DAG Execution Tests"
          
          # Test each DAG can be parsed and executed
          dag_list="${{ needs.dag-syntax-validation.outputs.dag-list }}"
          
          if [ -n "$dag_list" ]; then
            IFS=',' read -ra DAGS <<< "$dag_list"
            for dag_id in "${DAGS[@]}"; do
              if [[ "$dag_id" =~ ${{ inputs.dag_filter || '.*' }} ]]; then
                echo "Testing DAG: $dag_id"
                
                # Test DAG can be triggered
                uv run airflow dags test "$dag_id" 2023-01-01 || echo "Warning: DAG $dag_id test failed"
                
                # List tasks in DAG
                echo "Tasks in $dag_id:"
                uv run airflow tasks list "$dag_id"
              fi
            done
          else
            echo "No DAGs to test"
          fi
          
          echo "::endgroup::"
          
        env:
          AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@localhost:5432/airflow

  # ==========================================================================
  # DAG SECURITY AND BEST PRACTICES JOB
  # ==========================================================================
  
  dag-security-analysis:
    name: DAG Security Analysis
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: dag-syntax-validation
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          
      - name: Install UV and security tools
        uses: astral-sh/setup-uv@v3
        with:
          enable-cache: true
          
      - name: Install dependencies
        run: |
          uv sync --dev
          uv add bandit[toml]
          uv add safety
          
      - name: Security scan of DAG files
        run: |
          echo "::group::Bandit Security Scan"
          
          # Scan DAG files for security issues
          if [ -d "airflow/dags" ] || [ -d "dags" ]; then
            uv run bandit -r airflow/dags/ dags/ -f json -o dag-security-report.json 2>/dev/null || true
            uv run bandit -r airflow/dags/ dags/ -f txt || true
          else
            echo "No DAG directories found to scan"
          fi
          
          echo "::endgroup::"
          
      - name: Check for hardcoded secrets
        run: |
          echo "::group::Hardcoded Secrets Check"
          
          # Check for potential hardcoded secrets in DAG files
          secret_patterns=(
            "password\s*=\s*['\"][^'\"]+['\"]"
            "api_key\s*=\s*['\"][^'\"]+['\"]"
            "secret\s*=\s*['\"][^'\"]+['\"]"
            "token\s*=\s*['\"][^'\"]+['\"]"
            "aws_access_key_id\s*=\s*['\"][^'\"]+['\"]"
            "aws_secret_access_key\s*=\s*['\"][^'\"]+['\"]"
          )
          
          found_secrets=false
          for pattern in "${secret_patterns[@]}"; do
            if grep -r -E -i "$pattern" airflow/dags/ dags/ 2>/dev/null; then
              found_secrets=true
              echo "âš ï¸  Potential hardcoded secret found matching pattern: $pattern"
            fi
          done
          
          if [ "$found_secrets" = true ]; then
            echo "âŒ Hardcoded secrets detected in DAG files"
            echo "Use Airflow Connections, Variables, or environment variables instead"
            exit 1
          else
            echo "âœ… No hardcoded secrets detected"
          fi
          
          echo "::endgroup::"
          
      - name: DAG best practices check
        run: |
          echo "::group::DAG Best Practices Check"
          
          # Check for common DAG anti-patterns
          uv run python -c "
          import os
          import ast
          import sys
          from pathlib import Path
          
          def check_dag_file(file_path):
              with open(file_path, 'r') as f:
                  content = f.read()
              
              issues = []
              
              # Check for datetime.now() usage (should use execution_date)
              if 'datetime.now()' in content:
                  issues.append('Uses datetime.now() - use context[\"execution_date\"] instead')
              
              # Check for time.sleep() in tasks
              if 'time.sleep(' in content:
                  issues.append('Uses time.sleep() - consider using TimeDeltaSensor instead')
              
              # Check for subprocess.call without proper error handling
              if 'subprocess.call(' in content and 'check=True' not in content:
                  issues.append('Uses subprocess.call() without check=True')
              
              # Check for Variable.get() in DAG definition (should be in task)
              if 'Variable.get(' in content and 'def ' not in content.split('Variable.get(')[0].split('\n')[-1]:
                  issues.append('Uses Variable.get() in DAG definition - move to task function')
              
              return issues
          
          dag_dirs = ['airflow/dags', 'dags']
          total_issues = 0
          
          for dag_dir in dag_dirs:
              if os.path.exists(dag_dir):
                  for dag_file in Path(dag_dir).rglob('*.py'):
                      issues = check_dag_file(dag_file)
                      if issues:
                          print(f'Issues in {dag_file}:')
                          for issue in issues:
                              print(f'  - {issue}')
                          total_issues += len(issues)
          
          if total_issues > 0:
              print(f'Found {total_issues} best practice violations')
              # Don't fail on best practice violations, just warn
          else:
              print('âœ… No best practice violations found')
          "
          
          echo "::endgroup::"
          
      - name: Upload security reports
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: dag-security-reports
          path: |
            dag-security-report.json
          retention-days: 30

  # ==========================================================================
  # DAG PERFORMANCE ANALYSIS JOB
  # ==========================================================================
  
  dag-performance-analysis:
    name: DAG Performance Analysis
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: [dag-syntax-validation]
    if: needs.dag-syntax-validation.outputs.dag-count > 0 && github.event_name != 'schedule'
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          
      - name: Install UV and Airflow
        uses: astral-sh/setup-uv@v3
        with:
          enable-cache: true
          
      - name: Install dependencies
        run: |
          uv sync --dev
          uv add "apache-airflow==${{ inputs.airflow_version || '3.0.1' }}"
          
      - name: Initialize Airflow
        run: |
          mkdir -p $AIRFLOW_HOME/dags $AIRFLOW_HOME/logs
          
          if [ -d "airflow/dags" ]; then
            cp -r airflow/dags/* $AIRFLOW_HOME/dags/
          fi
          
          uv run airflow db init
          
      - name: Analyze DAG complexity
        run: |
          echo "::group::DAG Complexity Analysis"
          
          uv run python -c "
          from airflow.models import DagBag
          import json
          
          dag_bag = DagBag(dag_folder='$AIRFLOW_HOME/dags', include_examples=False)
          analysis_results = {}
          
          for dag_id, dag in dag_bag.dags.items():
              task_count = len(dag.task_ids)
              
              # Calculate dependency complexity
              total_dependencies = sum(len(task.upstream_task_ids) for task in dag.tasks)
              
              # Estimate DAG depth (longest path)
              def calculate_depth(task, visited=None):
                  if visited is None:
                      visited = set()
                  if task.task_id in visited:
                      return 0
                  visited.add(task.task_id)
                  if not task.upstream_task_ids:
                      return 1
                  return 1 + max(calculate_depth(dag.get_task(upstream_id), visited.copy()) 
                                for upstream_id in task.upstream_task_ids)
              
              max_depth = max((calculate_depth(task) for task in dag.tasks), default=0)
              
              # Calculate parallelism potential
              leaf_tasks = [task for task in dag.tasks if not task.downstream_task_ids]
              root_tasks = [task for task in dag.tasks if not task.upstream_task_ids]
              
              analysis = {
                  'task_count': task_count,
                  'dependency_count': total_dependencies,
                  'max_depth': max_depth,
                  'root_tasks': len(root_tasks),
                  'leaf_tasks': len(leaf_tasks),
                  'complexity_score': task_count + (total_dependencies * 0.5) + (max_depth * 2)
              }
              
              analysis_results[dag_id] = analysis
              
              print(f'DAG: {dag_id}')
              print(f'  Tasks: {task_count}')
              print(f'  Dependencies: {total_dependencies}')
              print(f'  Max Depth: {max_depth}')
              print(f'  Complexity Score: {analysis[\"complexity_score\"]:.1f}')
              
              # Warnings for complex DAGs
              if task_count > 50:
                  print(f'  âš ï¸  High task count: {task_count}')
              if max_depth > 10:
                  print(f'  âš ï¸  Deep dependency chain: {max_depth}')
              if analysis['complexity_score'] > 100:
                  print(f'  âš ï¸  High complexity score: {analysis[\"complexity_score\"]:.1f}')
          
          # Save analysis results
          with open('dag-analysis.json', 'w') as f:
              json.dump(analysis_results, f, indent=2)
          "
          
          echo "::endgroup::"
          
      - name: Check DAG schedules and SLA
        run: |
          echo "::group::Schedule and SLA Analysis"
          
          uv run python -c "
          from airflow.models import DagBag
          from datetime import timedelta
          
          dag_bag = DagBag(dag_folder='$AIRFLOW_HOME/dags', include_examples=False)
          
          for dag_id, dag in dag_bag.dags.items():
              print(f'DAG: {dag_id}')
              print(f'  Schedule: {dag.schedule_interval}')
              print(f'  Catchup: {dag.catchup}')
              print(f'  Max Active Runs: {dag.max_active_runs}')
              
              # Check for SLA definitions
              sla_tasks = [task for task in dag.tasks if task.sla]
              if sla_tasks:
                  print(f'  Tasks with SLA: {len(sla_tasks)}')
                  for task in sla_tasks:
                      print(f'    {task.task_id}: {task.sla}')
              
              # Check for retries
              retry_tasks = [task for task in dag.tasks if task.retries and task.retries > 0]
              if retry_tasks:
                  print(f'  Tasks with retries: {len(retry_tasks)}')
              
              # Check for timeouts
              timeout_tasks = [task for task in dag.tasks if hasattr(task, 'execution_timeout') and task.execution_timeout]
              if timeout_tasks:
                  print(f'  Tasks with timeouts: {len(timeout_tasks)}')
              
              print()
          "
          
          echo "::endgroup::"
          
      - name: Upload performance analysis
        uses: actions/upload-artifact@v3
        with:
          name: dag-performance-analysis
          path: |
            dag-analysis.json
          retention-days: 30

  # ==========================================================================
  # DAG DOCUMENTATION VALIDATION JOB
  # ==========================================================================
  
  dag-documentation:
    name: DAG Documentation Validation
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: dag-syntax-validation
    if: needs.dag-syntax-validation.outputs.dag-count > 0
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          
      - name: Install UV and Airflow
        uses: astral-sh/setup-uv@v3
        with:
          enable-cache: true
          
      - name: Install dependencies
        run: |
          uv sync --dev
          uv add "apache-airflow==${{ inputs.airflow_version || '3.0.1' }}"
          
      - name: Initialize Airflow
        run: |
          mkdir -p $AIRFLOW_HOME/dags
          
          if [ -d "airflow/dags" ]; then
            cp -r airflow/dags/* $AIRFLOW_HOME/dags/
          fi
          
          uv run airflow db init
          
      - name: Validate DAG documentation
        run: |
          echo "::group::DAG Documentation Validation"
          
          uv run python -c "
          from airflow.models import DagBag
          import sys
          
          dag_bag = DagBag(dag_folder='$AIRFLOW_HOME/dags', include_examples=False)
          
          documentation_issues = []
          
          for dag_id, dag in dag_bag.dags.items():
              print(f'Checking documentation for DAG: {dag_id}')
              
              # Check DAG-level documentation
              if not dag.doc_md and not dag.description:
                  documentation_issues.append(f'{dag_id}: Missing DAG documentation (doc_md or description)')
              
              if not dag.tags:
                  documentation_issues.append(f'{dag_id}: Missing tags for categorization')
              
              if not dag.owner or dag.owner == 'airflow':
                  documentation_issues.append(f'{dag_id}: Missing or default owner')
              
              # Check task-level documentation
              undocumented_tasks = []
              for task in dag.tasks:
                  if not task.doc_md and not task.doc and not task.__class__.__doc__:
                      undocumented_tasks.append(task.task_id)
              
              if undocumented_tasks:
                  if len(undocumented_tasks) > len(dag.tasks) * 0.5:  # More than 50% undocumented
                      documentation_issues.append(f'{dag_id}: Majority of tasks lack documentation ({len(undocumented_tasks)}/{len(dag.tasks)})')
          
          # Report documentation issues
          if documentation_issues:
              print('âš ï¸  Documentation issues found:')
              for issue in documentation_issues:
                  print(f'  - {issue}')
              print()
              print('Consider adding:')
              print('  - DAG descriptions and doc_md')
              print('  - Task documentation')
              print('  - Meaningful tags')
              print('  - Proper ownership')
          else:
              print('âœ… All DAGs have adequate documentation')
          "
          
          echo "::endgroup::"
          
      - name: Generate DAG documentation
        run: |
          echo "::group::DAG Documentation Generation"
          
          uv run python -c "
          from airflow.models import DagBag
          import json
          from datetime import datetime
          
          dag_bag = DagBag(dag_folder='$AIRFLOW_HOME/dags', include_examples=False)
          
          documentation = {
              'generated_at': datetime.now().isoformat(),
              'airflow_version': '${{ inputs.airflow_version || \"3.0.1\" }}',
              'project': 'games',
              'dags': {}
          }
          
          for dag_id, dag in dag_bag.dags.items():
              dag_doc = {
                  'dag_id': dag_id,
                  'description': dag.description or 'No description provided',
                  'owner': dag.owner,
                  'tags': list(dag.tags) if dag.tags else [],
                  'schedule_interval': str(dag.schedule_interval),
                  'start_date': dag.start_date.isoformat() if dag.start_date else None,
                  'catchup': dag.catchup,
                  'max_active_runs': dag.max_active_runs,
                  'task_count': len(dag.task_ids),
                  'tasks': []
              }
              
              for task in dag.tasks:
                  task_doc = {
                      'task_id': task.task_id,
                      'task_type': task.__class__.__name__,
                      'upstream_tasks': list(task.upstream_task_ids),
                      'downstream_tasks': list(task.downstream_task_ids),
                      'retries': task.retries,
                      'retry_delay': str(task.retry_delay) if task.retry_delay else None,
                      'pool': task.pool if hasattr(task, 'pool') else None
                  }
                  dag_doc['tasks'].append(task_doc)
              
              documentation['dags'][dag_id] = dag_doc
          
          # Save documentation
          with open('dag-documentation.json', 'w') as f:
              json.dump(documentation, f, indent=2, default=str)
          
          print(f'Generated documentation for {len(dag_bag.dags)} DAGs')
          "
          
          echo "::endgroup::"
          
      - name: Upload DAG documentation
        uses: actions/upload-artifact@v3
        with:
          name: dag-documentation
          path: |
            dag-documentation.json
          retention-days: 90

  # ==========================================================================
  # INTEGRATION TESTING JOB
  # ==========================================================================
  
  integration-testing:
    name: Integration Testing
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: [dag-syntax-validation, dag-testing]
    if: |
      needs.dag-syntax-validation.outputs.dag-count > 0 && 
      (inputs.test_mode == 'full' || inputs.test_mode == 'integration-only')
    
    services:
      postgres:
        image: postgres:16
        env:
          POSTGRES_USER: airflow
          POSTGRES_PASSWORD: airflow
          POSTGRES_DB: airflow
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
          
      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          
      - name: Install UV and Airflow with extras
        uses: astral-sh/setup-uv@v3
        with:
          enable-cache: true
          
      - name: Install full Airflow stack
        run: |
          uv sync --dev
          uv add "apache-airflow[postgres,redis,celery]==${{ inputs.airflow_version || '3.0.1' }}"
          uv add apache-airflow-providers-postgres
          uv add apache-airflow-providers-redis
          uv add apache-airflow-providers-common-sql
          uv add apache-airflow-providers-fab
          uv add celery[redis]
          
      - name: Configure Airflow for integration testing
        run: |
          # Advanced Airflow 3.x configuration with PostgreSQL 16
          export AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="postgresql+psycopg2://airflow:airflow@localhost:5432/airflow"
          export AIRFLOW__CORE__EXECUTOR="CeleryExecutor"
          export AIRFLOW__CELERY__BROKER_URL="redis://localhost:6379/0"
          export AIRFLOW__CELERY__RESULT_BACKEND="db+postgresql+psycopg2://airflow:airflow@localhost:5432/airflow"
          
          # Create directories
          mkdir -p $AIRFLOW_HOME/dags $AIRFLOW_HOME/logs $AIRFLOW_HOME/plugins
          
          # Copy project files
          if [ -d "airflow/dags" ]; then
            cp -r airflow/dags/* $AIRFLOW_HOME/dags/
          fi
          if [ -d "airflow/plugins" ]; then
            cp -r airflow/plugins/* $AIRFLOW_HOME/plugins/
          fi
          
          # Initialize database
          uv run airflow db init
          
          # Create admin user for Airflow 3.x
          uv run airflow users create \
            --username admin \
            --firstname Admin \
            --lastname User \
            --role Admin \
            --email admin@games.com \
            --password admin
          
        env:
          AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@localhost:5432/airflow
          AIRFLOW__CORE__EXECUTOR: CeleryExecutor
          AIRFLOW__CELERY__BROKER_URL: redis://localhost:6379/0
          AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql+psycopg2://airflow:airflow@localhost:5432/airflow
          
      - name: Start Airflow components
        run: |
          # Start Airflow scheduler in background (Airflow 3.x)
          AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="postgresql+psycopg2://airflow:airflow@localhost:5432/airflow" \
          AIRFLOW__CORE__EXECUTOR="CeleryExecutor" \
          AIRFLOW__CELERY__BROKER_URL="redis://localhost:6379/0" \
          AIRFLOW__CELERY__RESULT_BACKEND="db+postgresql+psycopg2://airflow:airflow@localhost:5432/airflow" \
          uv run airflow scheduler &
          
          # Start Celery worker in background (with Redis support)
          AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="postgresql+psycopg2://airflow:airflow@localhost:5432/airflow" \
          AIRFLOW__CORE__EXECUTOR="CeleryExecutor" \
          AIRFLOW__CELERY__BROKER_URL="redis://localhost:6379/0" \
          AIRFLOW__CELERY__RESULT_BACKEND="db+postgresql+psycopg2://airflow:airflow@localhost:5432/airflow" \
          uv run airflow celery worker &
          
          # Wait for components to start
          sleep 30
          
      - name: Run integration tests
        run: |
          echo "::group::Integration Tests"
          
          # Test DAG triggering and execution
          dag_list="${{ needs.dag-syntax-validation.outputs.dag-list }}"
          
          if [ -n "$dag_list" ]; then
            IFS=',' read -ra DAGS <<< "$dag_list"
            for dag_id in "${DAGS[@]}"; do
              if [[ "$dag_id" =~ ${{ inputs.dag_filter || '.*' }} ]]; then
                echo "Testing DAG execution: $dag_id"
                
                # Trigger DAG run
                run_id=$(uv run airflow dags trigger "$dag_id" --output json | jq -r '.run_id' || echo "manual_$(date +%s)")
                
                if [ "$run_id" != "null" ] && [ -n "$run_id" ]; then
                  echo "Triggered DAG run: $run_id"
                  
                  # Wait for a short time and check status
                  sleep 10
                  
                  # Check DAG run status
                  uv run airflow dags state "$dag_id" 2023-01-01 || echo "Warning: Could not check DAG state"
                else
                  echo "Warning: Failed to trigger DAG $dag_id"
                fi
              fi
            done
          fi
          
          echo "::endgroup::"
          
        env:
          AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@localhost:5432/airflow
          AIRFLOW__CORE__EXECUTOR: CeleryExecutor
          AIRFLOW__CELERY__BROKER_URL: redis://localhost:6379/0
          AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql+psycopg2://airflow:airflow@localhost:5432/airflow
          
      - name: Test Airflow API
        run: |
          echo "::group::Airflow API Tests"
          
          # Start webserver for API testing (Airflow 3.x)
          AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="postgresql+psycopg2://airflow:airflow@localhost:5432/airflow" \
          AIRFLOW__WEBSERVER__WEB_SERVER_PORT=8080 \
          uv run airflow webserver --port 8080 &
          
          # Wait for webserver to start
          sleep 20
          
          # Test API endpoints (Airflow 3.x has enhanced API)
          echo "Testing Airflow API..."
          
          # Test health endpoint
          curl -f http://localhost:8080/health || echo "Health endpoint not available"
          
          # Test version endpoint (new in 3.x)
          curl -f http://localhost:8080/api/v1/version || echo "Version API endpoint requires authentication"
          
          # Test DAGs endpoint (if authentication allows)
          curl -f http://localhost:8080/api/v1/dags || echo "DAGs API endpoint requires authentication"
          
          echo "::endgroup::"
          
        env:
          AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@localhost:5432/airflow

  # ==========================================================================
  # SUMMARY AND REPORTING JOB
  # ==========================================================================
  
  validation-summary:
    name: Validation Summary
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: [dag-syntax-validation, dag-testing, dag-security-analysis, dag-performance-analysis, dag-documentation]
    if: always()
    
    steps:
      - name: Download all artifacts
        uses: actions/download-artifact@v3
        with:
          path: artifacts/
          
      - name: Generate validation summary
        run: |
          echo "# Airflow DAG Validation Summary" > validation-summary.md
          echo "" >> validation-summary.md
          echo "**Project:** games" >> validation-summary.md
          echo "**Date:** $(date -u +"%Y-%m-%d %H:%M:%S UTC")" >> validation-summary.md
          echo "**Commit:** ${{ github.sha }}" >> validation-summary.md
          echo "" >> validation-summary.md
          
          # Job status summary
          echo "## Job Status" >> validation-summary.md
          echo "" >> validation-summary.md
          echo "| Job | Status |" >> validation-summary.md
          echo "|-----|--------|" >> validation-summary.md
          echo "| DAG Syntax Validation | ${{ needs.dag-syntax-validation.result == 'success' && 'âœ… Passed' || 'âŒ Failed' }} |" >> validation-summary.md
          echo "| DAG Testing | ${{ needs.dag-testing.result == 'success' && 'âœ… Passed' || needs.dag-testing.result == 'skipped' && 'â­ï¸ Skipped' || 'âŒ Failed' }} |" >> validation-summary.md
          echo "| Security Analysis | ${{ needs.dag-security-analysis.result == 'success' && 'âœ… Passed' || 'âŒ Failed' }} |" >> validation-summary.md
          echo "| Performance Analysis | ${{ needs.dag-performance-analysis.result == 'success' && 'âœ… Passed' || needs.dag-performance-analysis.result == 'skipped' && 'â­ï¸ Skipped' || 'âŒ Failed' }} |" >> validation-summary.md
          echo "| Documentation | ${{ needs.dag-documentation.result == 'success' && 'âœ… Passed' || needs.dag-documentation.result == 'skipped' && 'â­ï¸ Skipped' || 'âŒ Failed' }} |" >> validation-summary.md
          echo "" >> validation-summary.md
          
          # DAG statistics
          echo "## DAG Statistics" >> validation-summary.md
          echo "" >> validation-summary.md
          echo "- **Total DAGs:** ${{ needs.dag-syntax-validation.outputs.dag-count }}" >> validation-summary.md
          echo "- **Airflow Version:** ${{ inputs.airflow_version || '3.0.1' }}" >> validation-summary.md
          echo "- **Test Mode:** ${{ inputs.test_mode || 'full' }}" >> validation-summary.md
          echo "" >> validation-summary.md
          
          if [ "${{ needs.dag-syntax-validation.outputs.dag-count }}" -gt "0" ]; then
            echo "### Discovered DAGs" >> validation-summary.md
            echo "" >> validation-summary.md
            IFS=',' read -ra DAGS <<< "${{ needs.dag-syntax-validation.outputs.dag-list }}"
            for dag_id in "${DAGS[@]}"; do
              echo "- \`$dag_id\`" >> validation-summary.md
            done
            echo "" >> validation-summary.md
          fi
          
          # Recommendations
          echo "## Recommendations" >> validation-summary.md
          echo "" >> validation-summary.md
          
          if [ "${{ needs.dag-syntax-validation.result }}" != "success" ]; then
            echo "- âŒ Fix DAG syntax and import errors before proceeding" >> validation-summary.md
          fi
          
          if [ "${{ needs.dag-security-analysis.result }}" != "success" ]; then
            echo "- ðŸ”’ Address security issues identified in the security scan" >> validation-summary.md
          fi
          
          if [ "${{ needs.dag-documentation.result }}" != "success" ] && [ "${{ needs.dag-documentation.result }}" != "skipped" ]; then
            echo "- ðŸ“š Improve DAG documentation and add missing descriptions" >> validation-summary.md
          fi
          
          if [ "${{ needs.dag-syntax-validation.outputs.dag-count }}" == "0" ]; then
            echo "- ðŸ“ No DAGs found - ensure DAGs are properly placed in the dags/ directory" >> validation-summary.md
          fi
          
          echo "- âœ… Consider setting up scheduled validation runs for continuous monitoring" >> validation-summary.md
          echo "" >> validation-summary.md
          
          echo "---" >> validation-summary.md
          echo "*Generated by games Airflow DAG Validation Pipeline*" >> validation-summary.md
          
      - name: Upload validation summary
        uses: actions/upload-artifact@v3
        with:
          name: validation-summary
          path: validation-summary.md
          retention-days: 90
          
      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const summary = fs.readFileSync('validation-summary.md', 'utf8');
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: summary
            });

# ============================================================================
# WORKFLOW CONFIGURATION SUMMARY
# ============================================================================

# This Airflow DAG validation workflow provides:
#
# 1. Comprehensive DAG Validation:
#    - Python syntax checking
#    - Import validation with DagBag
#    - Structure and configuration validation
#    - Circular dependency detection
#
# 2. Security Analysis:
#    - Security vulnerability scanning with Bandit
#    - Hardcoded secret detection
#    - Best practices validation
#    - Anti-pattern identification
#
# 3. Performance Analysis:
#    - DAG complexity scoring
#    - Dependency analysis
#    - Schedule and SLA validation
#    - Resource usage estimation
#
# 4. Documentation Validation:
#    - DAG and task documentation coverage
#    - Metadata completeness checking
#    - Automated documentation generation
#
# 5. Integration Testing:
#    - Multi-service environment setup
#    - DAG execution testing
#    - API endpoint validation
#    - End-to-end workflow testing
#
# 6. Advanced Features:
#    - Matrix testing across Airflow versions
#    - PostgreSQL and Redis integration
#    - Celery executor testing
#    - Manual workflow dispatch with parameters
#    - Scheduled validation runs
#    - Comprehensive reporting and summaries
#
# Usage:
# - Automatically triggers on Airflow-related file changes
# - Manual execution with customizable parameters
# - Scheduled daily validation for continuous monitoring
# - Integration with pull request workflows
# - Comprehensive artifact retention for analysis
#
# The workflow is optimized for:
# - Fast feedback on syntax errors
# - Comprehensive security analysis
# - Performance impact assessment
# - Documentation quality assurance
# - Production readiness validation
                      undocumented_tasks.append(task.